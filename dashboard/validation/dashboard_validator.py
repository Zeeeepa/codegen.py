#!/usr/bin/env python3
"""
Comprehensive Dashboard Validation Script
Tests all dashboard functionality with real API integration and comprehensive validation.
"""

import sys
import os
import time
import json
import traceback
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
import logging

# Add parent directories to path
sys.path.append(os.path.dirname(os.path.dirname(__file__)))
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

from services.mock_api_service import get_mock_api, AgentRunStatus

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class DashboardValidator:
    """Comprehensive dashboard functionality validator"""
    
    def __init__(self):
        self.api = get_mock_api()
        self.test_results = []
        self.start_time = datetime.now()
        
    def log_test(self, test_name: str, success: bool, details: str = "", duration: float = 0):
        """Log test result"""
        result = {
            'test_name': test_name,
            'success': success,
            'details': details,
            'duration': duration,
            'timestamp': datetime.now().isoformat()
        }
        self.test_results.append(result)
        
        status = "âœ… PASS" if success else "âŒ FAIL"
        logger.info(f"{status} {test_name} ({duration:.3f}s) - {details}")
    
    def test_api_authentication(self) -> bool:
        """Test API authentication and basic connectivity"""
        start_time = time.time()
        
        try:
            user = self.api.get_current_user()
            
            if user and user.github_username:
                self.log_test(
                    "API Authentication",
                    True,
                    f"Authenticated as {user.github_username}",
                    time.time() - start_time
                )
                return True
            else:
                self.log_test(
                    "API Authentication",
                    False,
                    "No user data returned",
                    time.time() - start_time
                )
                return False
                
        except Exception as e:
            self.log_test(
                "API Authentication",
                False,
                f"Authentication failed: {str(e)}",
                time.time() - start_time
            )
            return False
    
    def test_organization_access(self) -> bool:
        """Test organization access and data retrieval"""
        start_time = time.time()
        
        try:
            orgs = self.api.get_organizations(limit=10)
            
            if orgs and len(orgs) > 0:
                org_names = [org.name for org in orgs]
                self.log_test(
                    "Organization Access",
                    True,
                    f"Found {len(orgs)} organizations: {', '.join(org_names)}",
                    time.time() - start_time
                )
                return True
            else:
                self.log_test(
                    "Organization Access",
                    False,
                    "No organizations found",
                    time.time() - start_time
                )
                return False
                
        except Exception as e:
            self.log_test(
                "Organization Access",
                False,
                f"Failed to get organizations: {str(e)}",
                time.time() - start_time
            )
            return False
    
    def test_agent_run_listing(self) -> bool:
        """Test agent run listing with various filters"""
        start_time = time.time()
        
        try:
            # Test basic listing
            runs = self.api.list_agent_runs(323, limit=10)
            
            if not runs or 'items' not in runs:
                self.log_test(
                    "Agent Run Listing",
                    False,
                    "No runs data structure returned",
                    time.time() - start_time
                )
                return False
            
            total_runs = len(runs['items'])
            
            # Test status filtering
            status_tests = []
            for status in AgentRunStatus:
                filtered_runs = self.api.list_agent_runs(323, limit=50, status=status.value)
                status_count = len(filtered_runs['items'])
                status_tests.append(f"{status.value}: {status_count}")
            
            # Test pagination
            page1 = self.api.list_agent_runs(323, limit=5, skip=0)
            page2 = self.api.list_agent_runs(323, limit=5, skip=5)
            
            pagination_works = (
                len(page1['items']) <= 5 and 
                len(page2['items']) <= 5 and
                page1['items'] != page2['items']
            )
            
            self.log_test(
                "Agent Run Listing",
                True,
                f"Found {total_runs} runs. Status breakdown: {', '.join(status_tests)}. Pagination: {'âœ“' if pagination_works else 'âœ—'}",
                time.time() - start_time
            )
            return True
            
        except Exception as e:
            self.log_test(
                "Agent Run Listing",
                False,
                f"Failed to list runs: {str(e)}",
                time.time() - start_time
            )
            return False
    
    def test_agent_run_creation(self) -> bool:
        """Test creating new agent runs"""
        start_time = time.time()
        
        try:
            test_prompt = f"Test run created by validator at {datetime.now().isoformat()}"
            metadata = {
                "source": "validator",
                "test": True,
                "priority": "low"
            }
            
            new_run = self.api.create_agent_run(
                323,
                test_prompt,
                metadata=metadata
            )
            
            if new_run and new_run.id:
                # Verify the run was created correctly
                retrieved_run = self.api.get_agent_run(323, new_run.id)
                
                if retrieved_run and retrieved_run.prompt == test_prompt:
                    self.log_test(
                        "Agent Run Creation",
                        True,
                        f"Created run #{new_run.id} successfully",
                        time.time() - start_time
                    )
                    return True
                else:
                    self.log_test(
                        "Agent Run Creation",
                        False,
                        f"Run created but retrieval failed or data mismatch",
                        time.time() - start_time
                    )
                    return False
            else:
                self.log_test(
                    "Agent Run Creation",
                    False,
                    "No run ID returned from creation",
                    time.time() - start_time
                )
                return False
                
        except Exception as e:
            self.log_test(
                "Agent Run Creation",
                False,
                f"Failed to create run: {str(e)}",
                time.time() - start_time
            )
            return False
    
    def test_agent_run_management(self) -> bool:
        """Test agent run management operations (pause, resume, cancel)"""
        start_time = time.time()
        
        try:
            # Create a test run for management operations
            test_run = self.api.create_agent_run(
                323,
                "Test run for management operations",
                metadata={"test": "management"}
            )
            
            if not test_run:
                self.log_test(
                    "Agent Run Management",
                    False,
                    "Failed to create test run for management",
                    time.time() - start_time
                )
                return False
            
            operations_tested = []
            
            # Test pause (if run is running)
            if test_run.status in ['running', 'pending']:
                pause_result = self.api.pause_agent_run(323, test_run.id)
                operations_tested.append(f"pause: {'âœ“' if pause_result else 'âœ—'}")
                
                if pause_result:
                    # Test resume
                    resume_result = self.api.resume_agent_run(323, test_run.id, "Resume test")
                    operations_tested.append(f"resume: {'âœ“' if resume_result else 'âœ—'}")
            
            # Test cancel
            cancel_result = self.api.cancel_agent_run(323, test_run.id)
            operations_tested.append(f"cancel: {'âœ“' if cancel_result else 'âœ—'}")
            
            # Verify final state
            final_run = self.api.get_agent_run(323, test_run.id)
            final_status_correct = final_run.status == 'cancelled'
            operations_tested.append(f"final_status: {'âœ“' if final_status_correct else 'âœ—'}")
            
            all_operations_passed = all('âœ“' in op for op in operations_tested)
            
            self.log_test(
                "Agent Run Management",
                all_operations_passed,
                f"Operations tested: {', '.join(operations_tested)}",
                time.time() - start_time
            )
            return all_operations_passed
            
        except Exception as e:
            self.log_test(
                "Agent Run Management",
                False,
                f"Management operations failed: {str(e)}",
                time.time() - start_time
            )
            return False
    
    def run_comprehensive_validation(self) -> Dict[str, Any]:
        """Run all validation tests"""
        logger.info("ğŸš€ Starting comprehensive dashboard validation...")
        logger.info(f"Validation started at: {self.start_time.isoformat()}")
        
        # Define test suite
        tests = [
            ("API Authentication", self.test_api_authentication),
            ("Organization Access", self.test_organization_access),
            ("Agent Run Listing", self.test_agent_run_listing),
            ("Agent Run Creation", self.test_agent_run_creation),
            ("Agent Run Management", self.test_agent_run_management),
        ]
        
        # Run all tests
        passed_tests = 0
        total_tests = len(tests)
        
        for test_name, test_func in tests:
            logger.info(f"Running test: {test_name}")
            try:
                if test_func():
                    passed_tests += 1
            except Exception as e:
                logger.error(f"Test {test_name} crashed: {str(e)}")
                logger.error(traceback.format_exc())
        
        # Calculate results
        end_time = datetime.now()
        total_duration = (end_time - self.start_time).total_seconds()
        success_rate = (passed_tests / total_tests) * 100
        
        # Generate summary
        summary = {
            "validation_start": self.start_time.isoformat(),
            "validation_end": end_time.isoformat(),
            "total_duration": total_duration,
            "total_tests": total_tests,
            "passed_tests": passed_tests,
            "failed_tests": total_tests - passed_tests,
            "success_rate": success_rate,
            "status": "PASS" if success_rate >= 80 else "FAIL",
            "detailed_results": self.test_results
        }
        
        # Log summary
        logger.info("=" * 60)
        logger.info("ğŸ“Š VALIDATION SUMMARY")
        logger.info("=" * 60)
        logger.info(f"Total Tests: {total_tests}")
        logger.info(f"Passed: {passed_tests}")
        logger.info(f"Failed: {total_tests - passed_tests}")
        logger.info(f"Success Rate: {success_rate:.1f}%")
        logger.info(f"Total Duration: {total_duration:.2f}s")
        logger.info(f"Overall Status: {summary['status']}")
        logger.info("=" * 60)
        
        if success_rate >= 80:
            logger.info("âœ… Dashboard validation PASSED! Ready for production use.")
        else:
            logger.warning("âš ï¸ Dashboard validation FAILED. Review failed tests before deployment.")
        
        return summary

def main():
    """Main validation function"""
    print("ğŸš€ Codegen Dashboard Comprehensive Validation")
    print("=" * 60)
    
    validator = DashboardValidator()
    summary = validator.run_comprehensive_validation()
    
    # Save results
    json_file = os.path.join(os.path.dirname(__file__), 'validation_results.json')
    with open(json_file, 'w') as f:
        json.dump(summary, f, indent=2, default=str)
    
    print(f"\nğŸ“„ Results saved to: {json_file}")
    
    return summary['status'] == 'PASS'

if __name__ == '__main__':
    success = main()
    sys.exit(0 if success else 1)

